# 分布式系统工程

## 1. 分布式系统简介：

### 1.1 什么是分布式系统：

* 系统拥有多台互相合作的电脑。
* 系统或者是用来存储大型网站的数据，或者是用来做map-reduce，或者是用来做点对点的共享等等。
* 系统的绝大部分基础设施是分布式的。

### 1.2 我们为什么要创建分布式系统：

* 可以通过并行增加系统的性能。
* 可以通过复制增加系统的容错。
* 可以将计算放在离外部实体更近的地方（从而可以加快系统的计算速度）。
* 可以通过隔离增加系统的安全。

### 1.3 分布式系统的挑战：

* 系统有许多并发的部分。
* 系统内的相互作用非常复杂。
* 必须应付系统内部的局部失败。
* 很难看清系统的性能潜力。

## 2. 分布式系统的解决方案：

### 2.1 宏观目标：

我们需要提供一系列能够隐藏分布式系统复杂性的抽象。

### 2.2 研究角度：

* 实现方式。
    * RPC, 线程和并发控制。
* 性能：
    * 通常我们想要提供一个性能可以扩展的系统。
    * 可以通过简单增加系统内电脑数量来增强并行能力，从而部分扩展系统的性能。
        * 当没有复杂交互的时候这么做很有效。
        * 而且可以不用请昂贵的程序员来重新设计系统。
    * 但是简单增加系统内电脑数量并不能一直增加系统性能：
        * 当电脑数量变得很多的时候，负载不均，系统内每台电脑性能不均，无法并行执行的代码，初始化的交互都会降低系统的性能。
        * 来自共享资源的访问也会造成性能瓶颈，比如网络通讯。
    * 同时性能也并不能总是靠增加系统内电脑数量达成：
        * 比如来自单一用户请求的快速响应时间。
        * 比如所有用户都想要更新同一个数据。
        * 通常这些情况需要更好的程序设计而不是更多的电脑。
* 容错：
    * 大量的服务器+大型的网络通常代表着总有错误会发生。
    * 我们想要向应用程序隐藏这些错误。
    * 我们通常想要让系统拥有可用性和可恢复性。
        * 可用性：即使错误发生了，系统还是可以继续运行。
        * 可恢复性：当错误被修复之后，系统可以恢复运行。
    * 通常可以用备用的服务器来增加容错。
* 一致性：
    * 通常想要达成正确工作的系统十分困难：
        * 服务器和它的备份服务器之间很难保持一致。
        * 客户端可能会在中途出错。
        * 服务器可能会在处理之后回复之前崩溃。
        * 不佳的网络可能会使得正常的服务器不被采用。
    * 一致性和性能通常是矛盾的：
        * 高一致性需要各种基础设置之间大量的通信。
        * 许多设计为了提升性能被迫只提供低一致性。

## 3. 案例分析：Map-Reduce

### 3.1 MR简介：

![map reduce structure](https://github.com/eronekogin/mit_6_824/blob/master/map_reduce.png)

* MR用来处理在TB级别的数据集上的运算，比如创建搜索索引，排序，或者分析网络结构。
* 主要目标是可以让不熟悉分布式系统的程序员也能够轻松地定义map和reduce的函数，而MR则会处理与分布式系统相关的内部细节，同时向用户程序隐藏这些细节。


### 3.2 MR分析：

* MR拥有很好的扩展性：
    * 电脑的数量和系统的性能成正比。
    * 多个map函数可以并行执行，因为它们之间没有交互。reduce函数同理。
* MR系统隐藏了很多与分布式处理相关的细节：
    * 比如发送客户代码给服务器。
    * 追踪哪些任务已经完成。
    * 把数据从map函数送到reduce函数。
    * 服务器之间的负载均衡。
    * 从错误中恢复系统。
* MR同时限制了应用程序可以做的事情：
    * 没有状态的交互。
    * 没有迭代，也即是没有多个阶段的管道。
    * 没有实时或者流数据处理。
* MR的输入和输出都在GFS集群上：
    * 导致它需要大量并行的输入和输出的性能。
    * GFS将文件分割成64MB大小的块并存储在不同的服务器中：
        * map并行读取块。
        * reduce并行写入块。
    * GFS同时还将文件备份到2到3个服务器中。
* 对性能限制的因素：
    * 2004年谷歌写MR论文的时候当时主要的限制在于不同服务器之间的网络通信速度。

### 3.3 MR的一些工作细节：

* 系统只有一台主机用来派发任务给工作机，同时还要记住工作进度。
    * 主机派发map任务给工作机直到所有map任务完成。
    * map输出中间数据到本地磁盘，然后会根据中间数据的键的哈希值将其分割成不同文件，每个文件将会被一个reduce读取。
    * 当所有map任务完成之后，主机开始派发reduce任务给工作机。
    * 每个reduce从它所有的map的工作机获取中间数据作为它的输入，然后将输出存在GFS上一个单独的文件中。
* 为了尽量减少网络通信：
    * 主机尝试在存储输入文件块的GFS服务器上运行map任务：
        * 让所有的电脑同时执行GFS和MR工作进程。
        * 所以map的输入可以直接从本地读取（通过GFS），而不用通过网络。
    * 中间数据只会通过网络发送一次：
        * map工作机将输出存到本地磁盘。
        * reduce的工作机将会直接从map的工作机指定的位置读取中间数据，而不用通过GFS。
    * 中间数据根据键的哈希值被分到不同的文件中。
        * 相对于键的总数来说，reduce任务要少很多。
        * 所以通过网络传输是更有效的。
* 为了负载平衡：
    * 为了加快整体的运行时间而不是让系统里所有其他机器都等待着最后几台机器的处理完成，MR可以让已经完成任务的工作机也执行当前的任务，然后读取最快完成任务的那台机器的结果，而不用继续等待原来的机器的任务完成。
* 为了容错：
    * 当有工作机中途失败时，MR只需要重新执行失败的map和reduce的任务，而不需要重新执行整个job。
    * 为了达成这一点，MR要求map和reduce函数必须是确定的，也即是它们只能查看它们的输入参数，不能有状态，文件存取，交互或者是外部的通信之类的操作。
    * 如果不是这样类型的函数，则当有工作机失败时，整个job都要重新运行。
    * 当map工作机崩溃时：
        * 主机发现工作机不再响应。
        * 它会派发原来的map任务给其它工作机重新执行。
            * 如果中间数据已经被reduce工作机获取则不需要重新执行。
    * 当reduce工作机崩溃时：
        * 已经完成的任务不需要重复执行，因为结果已经被保存在GFS里。
        * 如果任务没有完成，则派发原来的reduce任务给其它工作机重新执行。
    * 如果主机发送了同样的map任务给了两个不同的工作机：
        * 它会在派发reduce任务的时候只选择其中一个中间数据。
    * 如果主机派发了同样的reduce任务给了两个不同的工作机：
        * GFS的重命名机制会将其中一个输出覆盖掉以保证最后只看到一个输出。
    * 如果其中一个工作机运行很慢：
        * 主机会将上几个任务重新交给其它工作机执行。
    * 如果工作机生因为硬件或者软件的损坏生成了错误的结果：
        * MR没有对应的处理方式。
    * 如果主机崩溃了：
        * 谷歌假设主机不太可能会崩溃。

### 3.4 MR的当前状态：

* MR的原理对业界仍然有很大的影响力，比如hadoop，spark等框架的建立。
* 谷歌可能不再使用它了：
    * MR被flume/flumejava替换。
    * GFS被Colossus和Big Table替换。

### 3.5 结论：

* MR仅凭一己之力就使得大型集群计算变得十分受欢迎：
    * 它并不是最有效或者最灵活的解决方案。
    * 它十分容易扩展。
    * 它十分容易编程，因为它隐藏了底层的数据移动和错误处理。


